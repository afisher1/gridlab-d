#!/usr/bin/env python3
"""
Web Page Crawler and Markdown Converter
Follows links from a specified page and saves content as Markdown files.

Generated by Claude Sonnet 4 on June 9, 2025 using the following
prompt written by Trevor Hardy:

"I need a Python script that will follow all the links from a 
specified page, go to each page in turns, copy the text off the page, 
and write it out locally to a text file in Markdown format."

Shoutwiki had anti-bot protection and so after asking Claude for 
recommendations on circumventing it, I took one of many of its 
suggestions and changed the headers on the HTTP request.

Reviewed and edited by Trevor Hardy

After running this script I (Trevor) removed the 
"gridlab-d_shoutwiki_com_wiki_" string with the following 
Linux one-liner run inside the "scraped_pages" directory: 
`for f in *; do mv "$f" "${f:29}"; done`
"""

import requests
from bs4 import BeautifulSoup
import html2text
import os
import time
import urllib.parse
from urllib.robotparser import RobotFileParser
import logging
from pathlib import Path
import re

class WebCrawler:
    def __init__(self, base_url, output_dir="crawled_pages", delay=1):
        self.base_url = base_url
        self.output_dir = Path(output_dir)
        self.delay = delay  # Delay between requests (be respectful)
        self.visited_urls = set()
        self.session = requests.Session()
        self.html_converter = html2text.HTML2Text()
        
        # Configure HTML to Markdown converter
        self.html_converter.ignore_links = False
        self.html_converter.ignore_images = False
        self.html_converter.body_width = 0  # Don't wrap lines
        
        # Setup logging
        logging.basicConfig(level=logging.INFO, 
                          format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        # Create output directory
        self.output_dir.mkdir(exist_ok=True)
        
        # Set up session headers
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })

    def is_valid_url(self, url):
        """Check if URL is valid and within the same domain"""
        try:
            parsed_base = urllib.parse.urlparse(self.base_url)
            parsed_url = urllib.parse.urlparse(url)
            
            # Check if it's the same domain
            return (parsed_url.netloc == parsed_base.netloc or 
                   parsed_url.netloc == '' or
                   url.startswith('/'))
        except:
            return False

    def normalize_url(self, url):
        """Normalize URL to absolute form"""
        if url.startswith('http'):
            return url
        elif url.startswith('/'):
            parsed_base = urllib.parse.urlparse(self.base_url)
            return f"{parsed_base.scheme}://{parsed_base.netloc}{url}"
        else:
            return urllib.parse.urljoin(self.base_url, url)

    def sanitize_filename(self, url):
        """Create a safe filename from URL"""
        # Remove protocol and clean up
        filename = re.sub(r'https?://', '', url)
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        filename = filename.replace('.', '_')
        
        # Limit length and add extension
        if len(filename) > 100:
            filename = filename[:100]
        
        return f"{filename}.md"

    def extract_links(self, soup, current_url):
        """Extract all valid links from the page"""
        links = set()
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = self.normalize_url(href)
            
            if (self.is_valid_url(full_url) and 
                full_url not in self.visited_urls and
                not href.startswith('#') and  # Skip anchors
                not href.startswith('mailto:') and  # Skip email links
                not href.startswith('tel:')):  # Skip phone links
                links.add(full_url)
        
        return links

    def extract_content(self, soup):
        """Extract main content from the page"""
        # Try to find main content areas first
        content_selectors = [
            'main', 'article', '.content', '#content', 
            '.post', '.entry', '[role="main"]'
        ]
        
        content = None
        for selector in content_selectors:
            content = soup.select_one(selector)
            if content:
                break
        
        # If no main content area found, use body but remove navigation, footer, etc.
        if not content:
            content = soup.find('body')
            if content:
                # Remove common non-content elements
                for tag in content(['nav', 'header', 'footer', 'aside', 'script', 'style']):
                    tag.decompose()
        
        return content if content else soup

    def crawl_page(self, url):
        """Crawl a single page and return its content and links"""
        try:
            self.logger.info(f"Crawling: {url}")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract title
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "Untitled"
            
            # Extract main content
            content = self.extract_content(soup)
            
            # Convert to markdown
            markdown_content = self.html_converter.handle(str(content))
            
            # Add title and URL header
            full_markdown = f"# {title_text}\n\n"
            full_markdown += f"**Source URL:** {url}\n\n"
            full_markdown += f"---\n\n{markdown_content}"
            
            # Extract links for further crawling
            links = self.extract_links(soup, url)
            
            return full_markdown, links
            
        except Exception as e:
            self.logger.error(f"Error crawling {url}: {str(e)}")
            return None, set()

    def save_content(self, url, content):
        """Save content to a markdown file"""
        try:
            filename = self.sanitize_filename(url)
            filepath = self.output_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.logger.info(f"Saved: {filepath}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error saving content for {url}: {str(e)}")
            return False

    def crawl(self, max_pages=50):
        """Main crawling function"""
        urls_to_visit = [self.base_url]
        pages_crawled = 0
        
        while urls_to_visit and pages_crawled < max_pages:
            current_url = urls_to_visit.pop(0)
            
            if current_url in self.visited_urls:
                continue
            
            self.visited_urls.add(current_url)
            
            # Crawl the page
            content, new_links = self.crawl_page(current_url)
            
            if content:
                if self.save_content(current_url, content):
                    pages_crawled += 1
                
                # Add new links to the queue
                urls_to_visit.extend(list(new_links))
            
            # Be respectful - add delay between requests
            time.sleep(self.delay)
        
        self.logger.info(f"Crawling completed. {pages_crawled} pages saved to {self.output_dir}")

def main():
    """Main function with example usage"""
    # Configuration
    BASE_URL = "https://gridlab-d.shoutwiki.com/wiki/Index"
    OUTPUT_DIR = "scraped_pages"
    MAX_PAGES = 2000
    DELAY = 10
    
    # Create crawler instance
    crawler = WebCrawler(
        base_url=BASE_URL,
        output_dir=OUTPUT_DIR,
        delay=DELAY
    )
    
    # Start crawling
    print(f"\nStarting crawl of {BASE_URL}")
    print(f"Output directory: {OUTPUT_DIR}")
    print(f"Maximum pages: {MAX_PAGES}")
    print(f"Delay between requests: {DELAY} seconds")
    print("-" * 50)
    
    crawler.crawl(max_pages=MAX_PAGES)

if __name__ == "__main__":
    main()